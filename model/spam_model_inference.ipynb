{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e044130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import joblib\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Make output deterministic\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Ensure NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "processed_emails_csv_path = os.path.join(\".\", \"processed_emails_cleaned.csv\")\n",
    "suspicious_features_csv_path = os.path.join(\".\", \"suspicious_features_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4e65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text_for_spam(text):\n",
    "    \"\"\"Normalize whitespace, remove excessive line breaks, keep URLs and spam phrases.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_for_spam(text, use_stem=True):\n",
    "    \"\"\"Tokenize, keep URLs and spam words, stem/lemmatize if desired.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = []\n",
    "\n",
    "    for token in tokens:\n",
    "        token = token.strip(string.punctuation)\n",
    "        # Keep most tokens, including URLs, spam words, numbers\n",
    "        if len(token) <= 1:\n",
    "            continue\n",
    "        if use_stem:\n",
    "            token = stemmer.stem(token)\n",
    "        else:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "        cleaned.append(token)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d5fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_pipeline_path = \"spam_pipeline.pkl\"\n",
    "\n",
    "class SpamPipeline:\n",
    "    def __init__(self, vectorizer, model):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess(self, texts):\n",
    "        # Apply your existing cleaning + tokenization\n",
    "        return [\" \".join(tokenize_for_spam(clean_text_for_spam(text))) for text in texts]\n",
    "\n",
    "    def predict(self, texts):\n",
    "        processed = self.preprocess(texts)\n",
    "        X = self.vectorizer.transform(processed)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "pipeline_nb = joblib.load(spam_pipeline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0728a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Accuracy Naive Bayes 0.9333\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       1.00      0.87      0.93        15\n",
      "        SPAM       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  2]\n",
      " [ 0 15]]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Custom examples ----------\n",
    "\n",
    "spam_emails = [\n",
    "    \"Congratulations! You've won a $1000 gift card from Amazon! Click here to claim your reward before it expires.\",\n",
    "    \"Earn money from home in just 24 hours with our guaranteed system. No skills required, sign up now and start earning today!\",\n",
    "    \"Exclusive deal: Buy one, get two free on all luxury watches. Hurry, this offer is only available for the next 12 hours.\",\n",
    "    \"Your bank account has been compromised! Unauthorized login detected. Verify your identity immediately by clicking this secure link.\",\n",
    "    \"Get cheap prescription drugs without a prescription! Order online today and receive free overnight shipping.\",\n",
    "    \"You have been selected to receive a free iPhone 15 Pro Max! Claim yours now before stocks run out.\",\n",
    "    \"Work from home and make $5000 per week easily. Start your journey to financial freedom today!\",\n",
    "    \"Act now! Your credit card reward points are about to expire. Redeem them today for cash or exciting gifts.\",\n",
    "    \"Congratulations! Youâ€™ve been chosen as our lucky winner of a luxury Caribbean vacation. Click here to confirm your spot.\",\n",
    "    \"Increase your Bitcoin balance instantly with our automated trading bot. Start with just $100 and watch your money grow.\",\n",
    "    \"Dear user, your email storage is 98% full. Click this link to upgrade your account for free unlimited storage.\",\n",
    "    \"Get rich quick! Our proven online course has helped thousands of people quit their jobs and earn six figures. Enroll today.\",\n",
    "    \"Attention! Your PayPal account has been locked due to suspicious activity. Confirm your details immediately to restore access.\",\n",
    "    \"FREE membership upgrade! Unlock premium features today and connect with thousands of singles in your area.\",\n",
    "    \"Limited-time offer: Buy diet pills now and lose up to 20 pounds in one month without exercise!\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"Meeting rescheduled to 3 PM tomorrow. Please confirm if the new time works for you.\",\n",
    "    \"Can you review the attached report and provide your feedback by the end of the day?\",\n",
    "    \"Happy birthday! Wishing you a wonderful day filled with joy, laughter, and lots of cake.\",\n",
    "    \"Lunch plans for Friday? Thinking about trying the new Italian restaurant downtown.\",\n",
    "    \"Project deadline extended to next Monday, giving us more time to finalize the presentation.\",\n",
    "    \"Family reunion details attached. Let me know if youâ€™ll be able to make it this year.\",\n",
    "    \"Reminder: Doctor's appointment at 10 AM. Donâ€™t forget to bring your previous medical records.\",\n",
    "    \"The client has requested a few changes to the proposal. Letâ€™s discuss them in tomorrowâ€™s meeting.\",\n",
    "    \"Hope youâ€™re doing well! Just wanted to check if youâ€™re free this weekend for a quick catch-up.\",\n",
    "    \"Donâ€™t forget to bring the printed copies of the slides for the team review meeting tomorrow.\",\n",
    "    \"We need to finalize the venue for the upcoming workshop. Please share your suggestions.\",\n",
    "    \"Attached are the photos from last weekendâ€™s hiking trip. It was a great experience!\",\n",
    "    \"Please find the travel itinerary attached for our upcoming conference in Mumbai.\",\n",
    "    \"Reminder: The library books are due for return on Monday. Kindly renew them if needed.\",\n",
    "    \"Thank you for your help with the project last week. I really appreciate your support!\"\n",
    "]\n",
    "\n",
    "examples = spam_emails + ham_emails\n",
    "true_labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
    "\n",
    "preds = pipeline_nb.predict(examples)\n",
    "label_map = {0: \"HAM\", 1: \"SPAM\"}\n",
    "\n",
    "results = [label_map[p] for p in preds]\n",
    "\n",
    "acc = accuracy_score(true_labels, preds)\n",
    "print(f\"Example Accuracy Naive Bayes {acc:.4f}\")\n",
    "    \n",
    "# ðŸ”¹ More detailed metrics\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(true_labels, preds, target_names=[\"HAM\", \"SPAM\"]))\n",
    "\n",
    "# ðŸ”¹ Confusion matrix\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48062c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pdf(path, label_map={0:\"HAM\", 1:\"SPAM\"}):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF, preprocess, vectorize, and predict spam/ham.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to PDF file.\n",
    "        model: Trained classifier (e.g., svc_clf, nb_clf, lr_clf).\n",
    "        vectorizer: Trained TF-IDF vectorizer.\n",
    "        label_map (dict): Mapping from numeric labels to strings.\n",
    "        \n",
    "    Returns:\n",
    "        dict with raw prediction, label string, and extracted text length.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "            break\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to read PDF: {e}\")\n",
    "\n",
    "    # Preprocess\n",
    "    processed = \" \".join(pipeline_nb.preprocess([text])[0].split())\n",
    "\n",
    "    if not processed.strip():\n",
    "        return {\"error\": \"No valid text extracted from PDF\"}\n",
    "\n",
    "    # Predict using pipeline\n",
    "    pred = pipeline_nb.predict([processed])[0]  # wrap text in a list for single sample\n",
    "\n",
    "    return {\n",
    "        \"prediction\": int(pred),\n",
    "        \"label\": label_map[pred],\n",
    "        \"text_length\": len(processed),\n",
    "        \"preview\": processed[:200] + (\"...\" if len(processed) > 200 else \"\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1769c399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 1,\n",
       " 'label': 'SPAM',\n",
       " 'text_length': 1507,\n",
       " 'preview': 'congratul you ve won 1000 gift card from amazon click here to claim your reward befor it expir earn money from home in just 24 hour with our guarante system no skill requir sign up now and start earn ...'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pdf(\"test.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
