{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "007ae793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import email\n",
    "import string\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Make output deterministic\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Ensure NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "punctuations = list(string.punctuation)\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "processed_emails_csv_path = os.path.join(\".\", \"processed_emails_cleaned.csv\")\n",
    "suspicious_features_csv_path = os.path.join(\".\", \"suspicious_features_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2e1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def flatten_to_string(parts, prefer_html=True):\n",
    "    \"\"\"Flatten nested email parts into clean plain text, optionally preferring HTML.\"\"\"\n",
    "    texts = []\n",
    "\n",
    "    if isinstance(parts, str):\n",
    "        texts.append(parts)\n",
    "    elif isinstance(parts, list):\n",
    "        for part in parts:\n",
    "            texts += flatten_to_string(part, prefer_html)\n",
    "    elif hasattr(parts, \"get_payload\"):\n",
    "        try:\n",
    "            ctype = parts.get_content_type()\n",
    "            payload = parts.get_payload(decode=True)\n",
    "            charset = parts.get_content_charset() or \"utf-8\"\n",
    "\n",
    "            if payload:\n",
    "                if isinstance(payload, (bytes, bytearray)):\n",
    "                    payload = payload.decode(charset, errors=\"ignore\")\n",
    "\n",
    "                if ctype == \"text/html\" and prefer_html:\n",
    "                    soup = BeautifulSoup(payload, \"lxml\")\n",
    "                    for s in soup([\"script\", \"style\"]):\n",
    "                        s.decompose()\n",
    "                    text = soup.get_text(separator=\" \", strip=True)\n",
    "                    if text:\n",
    "                        texts.append(text)\n",
    "                elif ctype == \"text/plain\" and not prefer_html:\n",
    "                    texts.append(payload)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return texts\n",
    "\n",
    "def clean_text_for_spam(text):\n",
    "    \"\"\"Normalize whitespace, remove excessive line breaks, keep URLs and spam phrases.\"\"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_email_text(path):\n",
    "    \"\"\"Read email file and return subject + cleaned body text.\"\"\"\n",
    "    with open(path, errors=\"ignore\") as f:\n",
    "        msg = email.message_from_file(f)\n",
    "\n",
    "    subject = msg.get(\"Subject\") or \"\"\n",
    "    payload = msg.get_payload()\n",
    "    body = \" \".join(flatten_to_string(payload, prefer_html=True))\n",
    "    body = clean_text_for_spam(body)\n",
    "\n",
    "    return (subject + \" \" + body).strip()\n",
    "\n",
    "def tokenize_for_spam(text, use_stem=True):\n",
    "    \"\"\"Tokenize, keep URLs and spam words, stem/lemmatize if desired.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    cleaned = []\n",
    "\n",
    "    for token in tokens:\n",
    "        token = token.strip(string.punctuation)\n",
    "        # Keep most tokens, including URLs, spam words, numbers\n",
    "        if len(token) <= 1:\n",
    "            continue\n",
    "        if use_stem:\n",
    "            token = stemmer.stem(token)\n",
    "        else:\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "        cleaned.append(token)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db5218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Email counts (numeric target):\n",
      "1    49783\n",
      "0    25219\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 1: Load or build cleaned dataset ----------\n",
    "\n",
    "if not os.path.exists(processed_emails_csv_path):\n",
    "    index_file = \"index\"  # file with lines: <label> <filepath>\n",
    "    labels = {}\n",
    "    with open(index_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                label, filepath = parts\n",
    "                filename = os.path.basename(filepath)\n",
    "                labels[filename] = label\n",
    "\n",
    "    rows = []\n",
    "    for fname, label in tqdm(labels.items(), desc=\"Parsing emails\"):\n",
    "        file_path = os.path.join(\"data\", fname)  # adjust folder if needed\n",
    "        text = extract_email_text(file_path) or \"\"\n",
    "        tokens = tokenize_for_spam(text)\n",
    "        clean_text = \" \".join(tokens).strip()\n",
    "        if clean_text:\n",
    "            rows.append((clean_text, label))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"clean_text\", \"label\"])\n",
    "\n",
    "    # Map labels to numeric targets\n",
    "    df['target'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "    df = df.dropna(subset=['clean_text', 'target'])\n",
    "    df['target'] = df['target'].astype(int)\n",
    "\n",
    "    print(df.head())\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "    df.to_csv(processed_emails_csv_path, index=False)\n",
    "    print(f\"Saved cleaned dataset -> {processed_emails_csv_path}\")\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(processed_emails_csv_path)\n",
    "\n",
    "print(\"\\nEmail counts (numeric target):\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b9a1bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generic ciali brand qualiti do you feel the pr...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>typo in debian/readm hi ve just updat from the...</td>\n",
       "      <td>ham</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>authent viagra authent viagra mega authent dis...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nice talk with ya hey billi it was realli fun ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>or trembl stomach cramp troubl in sleep weak loos</td>\n",
       "      <td>spam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text label  target\n",
       "0  generic ciali brand qualiti do you feel the pr...  spam       1\n",
       "1  typo in debian/readm hi ve just updat from the...   ham       0\n",
       "2  authent viagra authent viagra mega authent dis...  spam       1\n",
       "3  nice talk with ya hey billi it was realli fun ...  spam       1\n",
       "4  or trembl stomach cramp troubl in sleep weak loos  spam       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "907fc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Step 2: Suspicious word features ----------\n",
    "\n",
    "bundle_path = \"dataset_bundle.pkl\"\n",
    "vectorizer_path = \"vectorizer.pkl\"\n",
    "\n",
    "def vectorize_emails(train_texts, test_texts):\n",
    "    # Fit only on training data\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1,3),\n",
    "        analyzer='word',\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(train_texts.dropna().str.lower())\n",
    "    joblib.dump(vectorizer, vectorizer_path, compress=(\"zlib\", 3))\n",
    "    X_test = vectorizer.transform(test_texts.dropna().str.lower())\n",
    "\n",
    "    return X_train, X_test, vectorizer\n",
    "\n",
    "if not os.path.exists(bundle_path) or not os.path.exists(vectorizer_path):\n",
    "    df = df.dropna()\n",
    "    train_texts, test_texts, y_train , y_test = train_test_split(\n",
    "        df[\"clean_text\"], df[\"target\"],\n",
    "        test_size=0.2,\n",
    "        stratify=df[\"target\"],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    X_train, X_test, vectorizer = vectorize_emails(train_texts, test_texts)\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "\n",
    "    joblib.dump(\n",
    "        {\n",
    "            \"X_train\": X_train,\n",
    "            \"y_train\": y_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "        },\n",
    "        bundle_path,\n",
    "        compress=(\"zlib\", 3)\n",
    "    )\n",
    "else:\n",
    "    bundle = joblib.load(bundle_path)\n",
    "    X_train = bundle[\"X_train\"]\n",
    "    y_train = bundle[\"y_train\"]\n",
    "    X_test = bundle[\"X_test\"]\n",
    "    y_test = bundle[\"y_test\"]\n",
    "    vectorizer = joblib.load(vectorizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c926bd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training dataset counts:\n",
      "0: 20175\n",
      "1: 20175\n"
     ]
    }
   ],
   "source": [
    "# ---------- Step 3: Balance only training data ----------\n",
    "\n",
    "ham_idx = np.where(y_train == 0)[0]\n",
    "spam_idx = np.where(y_train == 1)[0]\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "spam_downsampled_idx = np.random.choice(spam_idx, size=len(ham_idx), replace=False)\n",
    "\n",
    "balanced_idx = np.concatenate([ham_idx, spam_downsampled_idx])\n",
    "\n",
    "# Subset X_train and y_train\n",
    "X_train_balanced = X_train[balanced_idx, :]\n",
    "y_train_balanced = y_train[balanced_idx]\n",
    "\n",
    "# Shuffle\n",
    "shuffled_idx = np.random.permutation(len(y_train_balanced))\n",
    "X_train_balanced = X_train_balanced[shuffled_idx, :]\n",
    "y_train_balanced = y_train_balanced[shuffled_idx]\n",
    "\n",
    "# Display counts\n",
    "unique, counts = np.unique(y_train_balanced, return_counts=True)\n",
    "print(\"Balanced training dataset counts:\")\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"{u}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39d8dd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.9918005466302247\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      5044\n",
      "        spam       0.99      0.99      0.99      9957\n",
      "\n",
      "    accuracy                           0.99     15001\n",
      "   macro avg       0.99      0.99      0.99     15001\n",
      "weighted avg       0.99      0.99      0.99     15001\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[4975   69]\n",
      " [  54 9903]]\n"
     ]
    }
   ],
   "source": [
    "svc_clf = LinearSVC(random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
    "svc_clf.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred = svc_clf.predict(X_test)\n",
    "\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=['ham','spam']))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b66eafbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9626691553896407\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.90      1.00      0.95      5044\n",
      "        spam       1.00      0.95      0.97      9957\n",
      "\n",
      "    accuracy                           0.96     15001\n",
      "   macro avg       0.95      0.97      0.96     15001\n",
      "weighted avg       0.97      0.96      0.96     15001\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[5025   19]\n",
      " [ 541 9416]]\n"
     ]
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=['ham','spam']))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07627662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9806679554696354\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.96      0.97      5044\n",
      "        spam       0.98      0.99      0.99      9957\n",
      "\n",
      "    accuracy                           0.98     15001\n",
      "   macro avg       0.98      0.98      0.98     15001\n",
      "weighted avg       0.98      0.98      0.98     15001\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[4836  208]\n",
      " [  82 9875]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=0.1, penalty=\"l2\", max_iter=500, class_weight=\"balanced\")\n",
    "lr_clf.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, target_names=['ham','spam']))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09cd202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam_pipeline.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SpamPipeline:\n",
    "    def __init__(self, vectorizer, model):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess(self, texts):\n",
    "        # Apply your existing cleaning + tokenization\n",
    "        return [\" \".join(tokenize_for_spam(clean_text_for_spam(text))) for text in texts]\n",
    "\n",
    "    def predict(self, texts):\n",
    "        processed = self.preprocess(texts)\n",
    "        X = self.vectorizer.transform(processed)\n",
    "        return self.model.predict(X)\n",
    "\n",
    "pipeline_nb = SpamPipeline(vectorizer, nb_clf)\n",
    "joblib.dump(pipeline_nb, \"spam_pipeline.pkl\", compress=(\"zlib\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34f930e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predictions using SVC ---\n",
      "Example Accuracy SVC: 0.5333\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       1.00      0.07      0.12        15\n",
      "        SPAM       0.52      1.00      0.68        15\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.76      0.53      0.40        30\n",
      "weighted avg       0.76      0.53      0.40        30\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 1 14]\n",
      " [ 0 15]]\n",
      "\n",
      "--- Predictions using Naive Bayes ---\n",
      "Example Accuracy Naive Bayes: 0.9333\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       1.00      0.87      0.93        15\n",
      "        SPAM       0.88      1.00      0.94        15\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "Confusion matrix:\n",
      "[[13  2]\n",
      " [ 0 15]]\n",
      "\n",
      "--- Predictions using Logistic Regression ---\n",
      "Example Accuracy Logistic Regression: 0.5000\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         HAM       0.00      0.00      0.00        15\n",
      "        SPAM       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 0 15]\n",
      " [ 0 15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arch/miniconda3/envs/mlsec/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arch/miniconda3/envs/mlsec/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arch/miniconda3/envs/mlsec/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ---------- Custom examples ----------\n",
    "\n",
    "spam_emails = [\n",
    "    \"Congratulations! You've won a $1000 gift card from Amazon! Click here to claim your reward before it expires.\",\n",
    "    \"Earn money from home in just 24 hours with our guaranteed system. No skills required, sign up now and start earning today!\",\n",
    "    \"Exclusive deal: Buy one, get two free on all luxury watches. Hurry, this offer is only available for the next 12 hours.\",\n",
    "    \"Your bank account has been compromised! Unauthorized login detected. Verify your identity immediately by clicking this secure link.\",\n",
    "    \"Get cheap prescription drugs without a prescription! Order online today and receive free overnight shipping.\",\n",
    "    \"You have been selected to receive a free iPhone 15 Pro Max! Claim yours now before stocks run out.\",\n",
    "    \"Work from home and make $5000 per week easily. Start your journey to financial freedom today!\",\n",
    "    \"Act now! Your credit card reward points are about to expire. Redeem them today for cash or exciting gifts.\",\n",
    "    \"Congratulations! Youâ€™ve been chosen as our lucky winner of a luxury Caribbean vacation. Click here to confirm your spot.\",\n",
    "    \"Increase your Bitcoin balance instantly with our automated trading bot. Start with just $100 and watch your money grow.\",\n",
    "    \"Dear user, your email storage is 98% full. Click this link to upgrade your account for free unlimited storage.\",\n",
    "    \"Get rich quick! Our proven online course has helped thousands of people quit their jobs and earn six figures. Enroll today.\",\n",
    "    \"Attention! Your PayPal account has been locked due to suspicious activity. Confirm your details immediately to restore access.\",\n",
    "    \"FREE membership upgrade! Unlock premium features today and connect with thousands of singles in your area.\",\n",
    "    \"Limited-time offer: Buy diet pills now and lose up to 20 pounds in one month without exercise!\"\n",
    "]\n",
    "\n",
    "ham_emails = [\n",
    "    \"Meeting rescheduled to 3 PM tomorrow. Please confirm if the new time works for you.\",\n",
    "    \"Can you review the attached report and provide your feedback by the end of the day?\",\n",
    "    \"Happy birthday! Wishing you a wonderful day filled with joy, laughter, and lots of cake.\",\n",
    "    \"Lunch plans for Friday? Thinking about trying the new Italian restaurant downtown.\",\n",
    "    \"Project deadline extended to next Monday, giving us more time to finalize the presentation.\",\n",
    "    \"Family reunion details attached. Let me know if youâ€™ll be able to make it this year.\",\n",
    "    \"Reminder: Doctor's appointment at 10 AM. Donâ€™t forget to bring your previous medical records.\",\n",
    "    \"The client has requested a few changes to the proposal. Letâ€™s discuss them in tomorrowâ€™s meeting.\",\n",
    "    \"Hope youâ€™re doing well! Just wanted to check if youâ€™re free this weekend for a quick catch-up.\",\n",
    "    \"Donâ€™t forget to bring the printed copies of the slides for the team review meeting tomorrow.\",\n",
    "    \"We need to finalize the venue for the upcoming workshop. Please share your suggestions.\",\n",
    "    \"Attached are the photos from last weekendâ€™s hiking trip. It was a great experience!\",\n",
    "    \"Please find the travel itinerary attached for our upcoming conference in Mumbai.\",\n",
    "    \"Reminder: The library books are due for return on Monday. Kindly renew them if needed.\",\n",
    "    \"Thank you for your help with the project last week. I really appreciate your support!\"\n",
    "]\n",
    "\n",
    "examples = spam_emails + ham_emails\n",
    "true_labels = [1] * len(spam_emails) + [0] * len(ham_emails)\n",
    "\n",
    "processed_examples = [\" \".join(tokenize_for_spam(clean_text_for_spam(text))) for text in examples]\n",
    "\n",
    "X_examples = vectorizer.transform(processed_examples)\n",
    "\n",
    "# ---------- Define models ----------\n",
    "models = {\n",
    "    \"SVC\": svc_clf,\n",
    "    # \"Random Forest\": rf_clf,\n",
    "    \"Naive Bayes\": nb_clf,\n",
    "    \"Logistic Regression\": lr_clf,\n",
    "}\n",
    "\n",
    "label_map = {0: \"HAM\", 1: \"SPAM\"}\n",
    "\n",
    "# ---------- Predict & evaluate ----------\n",
    "for name, model in models.items():\n",
    "    preds = model.predict(X_examples)\n",
    "    results = [label_map[p] for p in preds]\n",
    "\n",
    "    print(f\"\\n--- Predictions using {name} ---\")\n",
    "    \n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    print(f\"Example Accuracy {name}: {acc:.4f}\")\n",
    "    \n",
    "    # ðŸ”¹ More detailed metrics\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(true_labels, preds, target_names=[\"HAM\", \"SPAM\"]))\n",
    "    \n",
    "    # ðŸ”¹ Confusion matrix\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13814e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNaive Bayes seems to perform the best on these custom examples, \\nachieving the highest accuracy and balanced precision/recall \\nfor both HAM and SPAM classes.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Naive Bayes seems to perform the best on these custom examples, \n",
    "achieving the highest accuracy and balanced precision/recall \n",
    "for both HAM and SPAM classes.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
